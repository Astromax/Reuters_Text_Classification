# Reuters_Text_Classification
This project explored the Reuters-21578 dataset of news articles.  In Part 0, simple statistics about the dataset, such as number of articles per category and the correlations among categories ("given that Article A has category X, what is the probability that it also has category Y?") were explored.  It was found that the "earn" category is by far the most common, most categories only had a handful of representative articles, and 20 of the 135 categories had zero representatives in the training set.
In Part 1, five simple sklearn classifiers were evaluated according to their ability to classify articles within the test set into each category.  This was done in a "1 vs all" manner, where each classifier was run 115 times, each time classifying all test set articles according to whether or not they have the particular category under investigation in that round.  The categories with zero training set representatives were excluded from this section because the sklearn classifiers need *something* to go on in order to create a model.  The classifiers were evaluated according to Accuracy, Precision, Recall, and F1 score.  For this type of data, Accuracy is a poor metric because the vast majority of articles are not going to be in whichever category is under consideration, so just guessing "0" for all categories all the time will still give a reasonably high score.  Precision & Recall are both "half-metrics", useful only really when paired together, as in the F1 score, which the metric used for final evaluation here.
In Part 2, a deep learning model, borrowed from Github user Jannenev (who worked on a variant of this dataset a couple years ago), was constructed, trained on about half the articles, and tested on a subset of the articles, again using Accuracy, Precision, Recall, and F1 score.  The train/val/test split was somewhat different in Part 2 vs Part 1, mainly because Part 2 required a val set while Part 1 did not.  Because of this difference in set splits, and because the DL model can handle categories which lack representatives without crashing, the DL model was evaluated on all 135 categories; this is why its metrics are in a separate table.
It was found that the Decision Tree Classifier was the best of the sklearners by a fair margin, and the Multinomial Naive Bayes Classifier was the worst, again by a fair margin.  The Deep Learning model achieved moderate success after training for only 25 epochs, but then it appeared to stall, not improving even slightly after another 100 epochs.  It is possible this was just a mistake in trying to continue training after having stopped it at 25 epochs, further investigation ought to reveal the answer pretty quickly.  As far as CNN vs Decision Tree, simply glancing at the performance plots shows the Decision Tree to be superior, but this is deceptive from a real-world perspective.  In order to run the Decision Tree Classifier on all categories would take somewhere in the neighborhood of 5 minutes, whereas the CNN could train more than 10 epochs per minute & run inference on the entire test set in seconds.  If one knows in advance that only one category is going to be checked, it might be better to use the Decision Tree, but realistically one will want to look for other category types at some point, and the DL model's ability to classify all categories at once is just going to be more useful.
In the future, I may try more complex models, longer continuous training periods, and hyperparameter optimization.  
